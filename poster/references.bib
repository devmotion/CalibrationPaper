@InProceedings{kumar18_train_calib_measur_neural_networ,
  author =       {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  title =        {Trainable Calibration Measures for Neural Networks
                  from Kernel Mean Embeddings},
  booktitle =    {Proceedings of the 35th International Conference on
                  Machine Learning},
  year =         2018,
  volume =       80,
  pages =        {2805--2814},
  abstract =     {Modern neural networks have recently been found to
                  be poorly calibrated, primarily in the direction of
                  over-confidence. Methods like entropy penalty and
                  temperature smoothing improve calibration by
                  clamping confidence, but in doing so compromise the
                  many legitimately confident predictions. We propose
                  a more principled fix that minimizes an explicit
                  calibration error during training. We present MMCE,
                  a RKHS kernel based measure of calibration that is
                  efficiently trainable alongside the negative
                  likelihood loss without careful hyper-parameter
                  tuning. Theoretically too, MMCE is a sound measure
                  of calibration that is minimized at perfect
                  calibration, and whose finite sample estimates are
                  consistent and enjoy fast convergence
                  rates. Extensive experiments on several network
                  architectures demonstrate that MMCE is a fast,
                  stable, and accurate method to minimize calibration
                  error while maximally preserving the number of high
                  confidence predictions.},
  pdf =
                  {http://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf},
  series =       {Proceedings of Machine Learning Research},
}

@InProceedings{vaicenavicius19_evaluat,
  author =       {Vaicenavicius, Juozas and Widmann, David and
                  Andersson, Carl and Lindsten, Fredrik and Roll,
                  Jacob and Sch\"{o}n, Thomas B.},
  title =        {Evaluating model calibration in classification},
  booktitle =    {Proceedings of Machine Learning Research},
  year =         2019,
  volume =       89,
  pages =        {3459--3467},
  abstract =     {Probabilistic classifiers output a probability
                  distribution on target classes rather than just a
                  class prediction. Besides providing a clear
                  separation of prediction and decision making, the
                  main advantage of probabilistic models is their
                  ability to represent uncertainty about
                  predictions. In safety-critical applications, it is
                  pivotal for a model to possess an adequate sense of
                  uncertainty, which for probabilistic classifiers
                  translates into outputting probability distributions
                  that are consistent with the empirical frequencies
                  observed from realized outcomes. A classifier with
                  such a property is called calibrated. In this work,
                  we develop a general theoretical calibration
                  evaluation framework grounded in probability theory,
                  and point out subtleties present in model
                  calibration evaluation that lead to refined
                  interpretations of existing evaluation
                  techniques. Lastly, we propose new ways to quantify
                  and visualize miscalibration in probabilistic
                  classification, including novel multidimensional
                  reliability diagrams.},
  month =        4,
  series =       {Proceedings of Machine Learning Research},
}