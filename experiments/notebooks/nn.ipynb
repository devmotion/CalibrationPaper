{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration error estimates and p-value approximations for modern neural networks\n",
    "### David Widmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In the following experiments we download a set of pretrained modern neural networks for\n",
    "the image data set CIFAR-10. We estimate the expected calibration error (ECE) with\n",
    "respect to the total variation distance and the squared kernel calibration error of\n",
    "these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "We perform distributed computing to speed up our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to activate the local package environment on all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivating\u001b[22m\u001b[39m environment at `~/Projects/julia/CalibrationPaper/experiments/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "    using Pkg\n",
    "    Pkg.activate(joinpath(@__DIR__, \"..\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the packages that are required on all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    using CalibrationErrors\n",
    "    using CalibrationPaper\n",
    "    using CalibrationTests\n",
    "\n",
    "    using DelimitedFiles\n",
    "    using Random\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following packages are only required on the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Conda\n",
    "using CSV\n",
    "using DataDeps\n",
    "using DataFrames\n",
    "using ProgressMeter\n",
    "using PyCall\n",
    "\n",
    "using LibGit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## Pretrained neural networks\n",
    "\n",
    "As a first step we download a set of pretrained neural networks for CIFAR-10 from\n",
    "[PyTorch-CIFAR10](https://github.com/huyvnphan/PyTorch-CIFAR10). We extract the\n",
    "predictions of these models on the validation data set together with the correct labels.\n",
    "First we check if the predictions and labels are already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{String,1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create directory for results\n",
    "const DATADIR = joinpath(@__DIR__, \"..\", \"data\", \"PyTorch-CIFAR10\")\n",
    "isdir(DATADIR) || mkpath(DATADIR)\n",
    "\n",
    "# check if predictions exist\n",
    "const ALL_MODELS = [\"densenet121\", \"densenet161\", \"densenet169\", \"googlenet\", \"inception_v3\",\n",
    "                    \"mobilenet_v2\", \"resnet_orig\", \"resnet18\", \"resnet34\", \"resnet50\",\n",
    "                    \"vgg11_bn\", \"vgg13_bn\", \"vgg16_bn\", \"vgg19_bn\"]\n",
    "const MISSING_MODELS = filter(ALL_MODELS) do name\n",
    "    !isfile(joinpath(DATADIR, \"$name.csv\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data does not exist, we start by loading all missing packages and\n",
    "registering the required data. If you want to rerun this experiment from\n",
    "scratch, please download the pretrained weights of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if !isempty(MISSING_MODELS) || !isfile(joinpath(DATADIR, \"labels.csv\"))   \n",
    "    # register the data source for the pretrained models\n",
    "    register(ManualDataDep(\n",
    "        \"PyTorch-CIFAR10\",\n",
    "        \"\"\"\n",
    "        Please go to\n",
    "            https://drive.google.com/drive/folders/15jBlLkOFg0eK-pwsmXoSesNDyDb_HOeV\n",
    "        and download the pretrained weights.\n",
    "        Note that this must be done manually since Google requires to confirm the download of\n",
    "        large files and I have not automated the confirmation process (yet).\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "    # install PyTorch\n",
    "    Conda.add(\"cpuonly=1.0\"; channel = \"pytorch\")\n",
    "    Conda.add(\"pytorch=1.3.0\"; channel = \"pytorch\")\n",
    "    Conda.add(\"torchvision=0.4.1\"; channel = \"pytorch\")\n",
    "\n",
    "    mktempdir() do dir\n",
    "        # clone the repository\n",
    "        repodir = mkdir(joinpath(dir, \"PyTorch-CIFAR10\"))\n",
    "        LibGit2.clone(\"https://github.com/huyvnphan/PyTorch-CIFAR10.git\", repodir)\n",
    "        LibGit2.checkout!(LibGit2.GitRepo(repodir), \"90325333f4da099b3a795693cfa18e64490dffe9\")\n",
    "\n",
    "        # copy pretrained weights to the correct directory\n",
    "        weightsdir = joinpath(repodir, \"models\", \"state_dicts\")\n",
    "        for name in MISSING_MODELS\n",
    "            cp(joinpath(datadep\"PyTorch-CIFAR10\", \"$name.pt\"), joinpath(weightsdir, \"$name.pt\"))\n",
    "        end\n",
    "\n",
    "        # load Python packages\n",
    "        torch = pyimport(\"torch\")\n",
    "        F = pyimport(\"torch.nn.functional\")\n",
    "        torchvision = pyimport(\"torchvision\")\n",
    "        transforms = pyimport(\"torchvision.transforms\")\n",
    "\n",
    "        # import local models\n",
    "        pushfirst!(PyVector(pyimport(\"sys\").\"path\"), repodir)\n",
    "        models = pyimport(\"models\")\n",
    "\n",
    "        # define transformation\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                                             [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "        # download CIFAR-10 validation data set\n",
    "        dataset = torchvision.datasets.CIFAR10(root=joinpath(dir, \"CIFAR10\"), train=false,\n",
    "                                               transform=transform, download=true)\n",
    "\n",
    "        # extract and save labels\n",
    "        if !isfile(joinpath(DATADIR, \"labels.csv\"))\n",
    "            @info \"extracting labels...\"\n",
    "\n",
    "            # extract labels of the validation data set\n",
    "            _, labels_py = iterate(torch.utils.data.DataLoader(dataset, batch_size=10_000, shuffle=false))[1]\n",
    "\n",
    "            # save labels (+1 since we need classes 1,...,n)\n",
    "            labels = pycall(labels_py.\"numpy\", PyArray) .+ 1\n",
    "            writedlm(joinpath(datadir, \"labels.csv\"), labels)\n",
    "        end\n",
    "\n",
    "        # extract predictions\n",
    "        if !isempty(MISSING_MODELS)\n",
    "            # create data loader with batches of 250 images\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=250, shuffle=false)\n",
    "\n",
    "            cd(repodir) do\n",
    "                @pywith torch.no_grad() begin\n",
    "                    for name in MISSING_MODELS\n",
    "                        @info \"extracting predictions of model $name...\"\n",
    "\n",
    "                        # load model with pretrained weights\n",
    "                        model = getproperty(models, name)(pretrained=true)\n",
    "\n",
    "                        # save all predictions\n",
    "                        open(joinpath(DATADIR, \"$name.csv\"), \"w\") do f\n",
    "                            for (images_py, _) in dataloader\n",
    "                                predictions = pycall(F.softmax(model(images_py), dim=1).\"numpy\", PyArray)\n",
    "                                writedlm(f, predictions, ',')\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        nothing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the true labels since they are the same for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Int64,1}:\n",
       "  4\n",
       "  9\n",
       "  9\n",
       "  1\n",
       "  7\n",
       "  7\n",
       "  2\n",
       "  7\n",
       "  4\n",
       "  2\n",
       "  1\n",
       " 10\n",
       "  6\n",
       "  ⋮\n",
       "  9\n",
       "  3\n",
       "  8\n",
       "  1\n",
       "  4\n",
       "  6\n",
       "  4\n",
       "  9\n",
       "  4\n",
       "  6\n",
       "  2\n",
       "  8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const LABELS = CSV.read(joinpath(DATADIR, \"labels.csv\");\n",
    "    header = false, delim = ',', type = Int) |> Matrix{Int} |> vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration error estimates\n",
    "\n",
    "For all pretrained neural networks we compute a set of different calibration error\n",
    "estimates and save them in a CSV file `errors.csv`. More concretely, we evaluate the\n",
    "expected calibration error estimators with 10 uniform bins per dimension and with data\n",
    "dependent bins, and the biased and the unbiased quadratic estimator of the squared\n",
    "kernel calibration error as well as the unbiased linear estimator for a uniformly scaled\n",
    "exponential kernel for which the bandwidth is set with the median heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: skipping calibration error estimation: output file /home/davwi492/Projects/julia/CalibrationPaper/experiments/notebooks/../data/PyTorch-CIFAR10/errors.csv exists\n",
      "└ @ Main In[8]:30\n"
     ]
    }
   ],
   "source": [
    "@everywhere function calibration_errors(rng::AbstractRNG, predictions, labels, channel)\n",
    "    # evaluate ECE estimators\n",
    "    ece_uniform = calibrationerror(ECE(UniformBinning(10)), predictions, labels)\n",
    "    put!(channel, true)\n",
    "    ece_dynamic = calibrationerror(ECE(MedianVarianceBinning(100)), predictions, labels)\n",
    "    put!(channel, true)\n",
    "\n",
    "    # compute kernel based on the median heuristic\n",
    "    kernel = median_TV_kernel(predictions)\n",
    "\n",
    "    # evaluate SKCE estimators\n",
    "    skceb_median = calibrationerror(BiasedSKCE(kernel), predictions, labels)\n",
    "    put!(channel, true)\n",
    "    skceuq_median = calibrationerror(QuadraticUnbiasedSKCE(kernel), predictions, labels)\n",
    "    put!(channel, true)\n",
    "    skceul_median = calibrationerror(LinearUnbiasedSKCE(kernel), predictions, labels)\n",
    "    put!(channel, true)\n",
    "\n",
    "    (\n",
    "        ECE_uniform = ece_uniform,\n",
    "        ECE_dynamic = ece_dynamic,\n",
    "        SKCEb_median = skceb_median,\n",
    "        SKCEuq_median = skceuq_median,\n",
    "        SKCEul_median = skceul_median\n",
    "    )\n",
    "end\n",
    "\n",
    "# do not recompute the calibration errors if a file with results exists\n",
    "if isfile(joinpath(DATADIR, \"errors.csv\"))\n",
    "    @info \"skipping calibration error estimation: output file $(joinpath(DATADIR, \"errors.csv\")) exists\"\n",
    "else\n",
    "    # define the pool of workers, the progress bar, and its update channel\n",
    "    wp = CachingPool(workers())\n",
    "    n = length(ALL_MODELS)\n",
    "    p = Progress(5 * n, 1, \"computing calibration error estimates...\")\n",
    "    channel = RemoteChannel(() -> Channel{Bool}(5 * n))\n",
    "\n",
    "    local estimates\n",
    "    @sync begin\n",
    "        # update the progress bar\n",
    "        @async while take!(channel)\n",
    "            next!(p)\n",
    "        end\n",
    "\n",
    "        # compute the p-value approximations for all models\n",
    "        estimates = let rng = Random.GLOBAL_RNG, datadir = DATADIR, labels = LABELS, channel = channel\n",
    "            pmap(wp, ALL_MODELS) do model\n",
    "                # load predictions\n",
    "                rawdata = CSV.read(joinpath(datadir, \"$model.csv\");\n",
    "                                   header = false, transpose = true, delim = ',',\n",
    "                                   type = Float64) |> Matrix{Float64}\n",
    "                predictions = [rawdata[:, i] for i in axes(rawdata, 2)]\n",
    "\n",
    "                # copy random number generator and set seed\n",
    "                _rng = deepcopy(rng)\n",
    "                Random.seed!(_rng, 1234)\n",
    "\n",
    "                # compute approximations\n",
    "                errors = calibration_errors(_rng, predictions, labels, channel)\n",
    "                merge((model = model,), errors)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # stop progress bar\n",
    "        put!(channel, false)\n",
    "    end\n",
    "\n",
    "    @info \"saving calibration error estimates...\"\n",
    "    CSV.write(joinpath(DATADIR, \"errors.csv\"), estimates)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration tests\n",
    "\n",
    "Additionally we compute different p-value approximations for each model. More concretely,\n",
    "we estimate the p-value by consistency resampling of the two ECE estimators mentioned\n",
    "above, by distribution-free bounds of the three SKCE estimators discussed above, and by\n",
    "the asymptotic approximations for the unbiased quadratic and linear SKCE estimators\n",
    "used above. The results are saved in a CSV file `pvalues.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: skipping p-value approximations: output file /home/davwi492/Projects/julia/CalibrationPaper/experiments/notebooks/../data/PyTorch-CIFAR10/pvalues.csv exists\n",
      "└ @ Main In[9]:45\n"
     ]
    }
   ],
   "source": [
    "@everywhere function calibration_pvalues(rng::AbstractRNG, predictions, labels, channel)\n",
    "    # evaluate consistency resampling based estimators\n",
    "    ece_uniform = ConsistencyTest(ECE(UniformBinning(10)), predictions, labels)\n",
    "    pvalue_ece_uniform = pvalue(ece_uniform; rng = rng)\n",
    "    put!(channel, true)\n",
    "    ece_dynamic = ConsistencyTest(ECE(MedianVarianceBinning(100)), predictions, labels)\n",
    "    pvalue_ece_dynamic = pvalue(ece_dynamic; rng = rng)\n",
    "    put!(channel, true)\n",
    "\n",
    "    # compute kernel based on the median heuristic\n",
    "    kernel = median_TV_kernel(predictions)\n",
    "\n",
    "    # evaluate distribution-free bounds\n",
    "    skceb_median_distribution_free = DistributionFreeTest(BiasedSKCE(kernel), predictions, labels)\n",
    "    pvalue_skceb_median_distribution_free = pvalue(skceb_median_distribution_free)\n",
    "    put!(channel, true)\n",
    "    skceuq_median_distribution_free = DistributionFreeTest(QuadraticUnbiasedSKCE(kernel), predictions, labels)\n",
    "    pvalue_skceuq_median_distribution_free = pvalue(skceuq_median_distribution_free)\n",
    "    put!(channel, true)\n",
    "    skceul_median_distribution_free = DistributionFreeTest(LinearUnbiasedSKCE(kernel), predictions, labels)\n",
    "    pvalue_skceul_median_distribution_free = pvalue(skceul_median_distribution_free)\n",
    "    put!(channel, true)\n",
    "\n",
    "    # evaluate asymptotic bounds\n",
    "    skceuq_median_asymptotic = AsymptoticQuadraticTest(kernel, predictions, labels)\n",
    "    pvalue_skceuq_median_asymptotic = pvalue(skceuq_median_asymptotic; rng = rng)\n",
    "    put!(channel, true)\n",
    "    skceul_median_asymptotic = AsymptoticLinearTest(kernel, predictions, labels)\n",
    "    pvalue_skceul_median_asymptotic = pvalue(skceul_median_asymptotic)\n",
    "    put!(channel, true)\n",
    "\n",
    "    (\n",
    "        ECE_uniform = pvalue_ece_uniform,\n",
    "        ECE_dynamic = pvalue_ece_dynamic,\n",
    "        SKCEb_median_distribution_free = pvalue_skceb_median_distribution_free,\n",
    "        SKCEuq_median_distribution_free = pvalue_skceuq_median_distribution_free,\n",
    "        SKCEul_median_distribution_free = pvalue_skceul_median_distribution_free,\n",
    "        SKCEuq_median_asymptotic = pvalue_skceuq_median_asymptotic,\n",
    "        SKCEul_median_asymptotic = pvalue_skceul_median_asymptotic\n",
    "    )\n",
    "end\n",
    "\n",
    "# do not recompute the p-values if a file with results exists\n",
    "if isfile(joinpath(DATADIR, \"pvalues.csv\"))\n",
    "    @info \"skipping p-value approximations: output file $(joinpath(DATADIR, \"pvalues.csv\")) exists\"\n",
    "else\n",
    "    # define the pool of workers, the progress bar, and its update channel\n",
    "    wp = CachingPool(workers())\n",
    "    n = length(ALL_MODELS)\n",
    "    p = Progress(7 * n, 1, \"computing p-value approximations...\")\n",
    "    channel = RemoteChannel(() -> Channel{Bool}(7 * n))\n",
    "\n",
    "    local estimates\n",
    "    @sync begin\n",
    "        # update the progress bar\n",
    "        @async while take!(channel)\n",
    "            next!(p)\n",
    "        end\n",
    "\n",
    "        # compute the p-value approximations for all models\n",
    "        estimates = let rng = Random.GLOBAL_RNG, datadir = DATADIR, labels = LABELS, channel = channel\n",
    "            pmap(wp, ALL_MODELS) do model\n",
    "                # load predictions\n",
    "                rawdata = CSV.read(joinpath(datadir, \"$model.csv\");\n",
    "                                   header = false, transpose = true, delim = ',',\n",
    "                                   type = Float64) |> Matrix{Float64}\n",
    "                predictions = [rawdata[:, i] for i in axes(rawdata, 2)]\n",
    "\n",
    "                # copy random number generator and set seed\n",
    "                _rng = deepcopy(rng)\n",
    "                Random.seed!(_rng, 1234)\n",
    "\n",
    "                # compute approximations\n",
    "                pvalues = calibration_pvalues(_rng, predictions, labels, channel)\n",
    "                merge((model = model,), pvalues)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # stop progress bar\n",
    "        put!(channel, false)\n",
    "    end\n",
    "\n",
    "    # save estimates\n",
    "    @info \"saving p-value approximations...\"\n",
    "    CSV.write(joinpath(DATADIR, \"pvalues.csv\"), estimates)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
